{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WknnrfgvN7a"
   },
   "source": [
    "# 과제#6. 영한 번역기 과제 \n",
    "\n",
    "- 코드 수정 내용 : \n",
    "1. Google Colab을 사용하기 위해 drive mount code를 추가하였습니다. 그리고 \"구어체(1).txt\"파일을 사용하기 위해 경로를 추가로 수정해주었습니다.\n",
    "2.  sentecnepiece 사용을 위해 sentecepiece라이브러리를 import해주었습니다.그리고 해당 라이브러리를 이용해서 32000개의 단어를 추출해주었고, 다시 그 단어를 \"구어체(2).txt\" 파일에 저장해두었습니다. 아래 구어체(2).txt파일 만들기에서 코드를 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlXH8ZIiwSip",
    "outputId": "84390c95-d02e-43fd-99bb-f0496387f791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "# mout\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSqEh5EaDSLp",
    "outputId": "51fd97e1-a0db-4e11-dd0e-a7c4eb3cb51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 디렉토리 위치: /content/drive/MyDrive/natural_language_processing_6\n"
     ]
    }
   ],
   "source": [
    "# txt 파일을 불러오기 위해 디렉토리 위치 이동\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# folder 변수에 구글드라이브에 프로젝트를 저장한 디렉토리를 입력하세요!\n",
    "project_dir = \"natural_language_processing_6\"\n",
    "\n",
    "base_path = Path(\"/content/drive/MyDrive/\")\n",
    "project_path = base_path / project_dir\n",
    "os.chdir(project_path)\n",
    "for x in list(project_path.glob(\"*\")):\n",
    "    if x.is_dir():\n",
    "        dir_name = str(x.relative_to(project_path))\n",
    "        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
    "print(f\"현재 디렉토리 위치: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation. This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
    "\n",
    "After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"*, and return the English translation: *\"are you still at home?\"*\n",
    "\n",
    "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
    "\n",
    "Note: This example takes approximately 10 minutes to run on a single P100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hWpLK2cCs9Z",
    "outputId": "8beb4112-ded9-4d46-b822-a94e2c625416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 31.0 MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20 kB 31.6 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30 kB 13.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40 kB 9.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 51 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 61 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71 kB 6.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 81 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 92 kB 7.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 102 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 112 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 122 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 133 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 143 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 153 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 163 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 174 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 184 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 194 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 204 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 215 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 225 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 235 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 245 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 256 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 266 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 276 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 286 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 296 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 307 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 317 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 327 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 337 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 348 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 358 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 368 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 378 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 389 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 399 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 409 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 419 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 430 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 440 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 450 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 460 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 471 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 481 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 491 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 501 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 512 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 522 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 532 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 542 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 552 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 563 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 573 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 583 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 593 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 604 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 614 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 624 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 634 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 645 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 655 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 665 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 675 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 686 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 696 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 706 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 716 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 727 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 737 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 747 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 757 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 768 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 778 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 788 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 798 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 808 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 819 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 829 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 839 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 849 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 860 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 870 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 880 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 890 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 901 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 911 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 921 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 931 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 942 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 952 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 962 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 972 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 983 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 993 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "# sentencepiece모델 설치\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sentencepiece as spm # sentecne piece 함수 사용을 위해 단어 추가 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# change path to bring file 구어체(1).txt\n",
    "path_to_file = \"/content/drive/MyDrive/natural_language_processing_6/구어체(1).txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsyI14Z7Rjl9"
   },
   "source": [
    "## sentencepiece를 이용해서 구어체(2).txt파일 만들어주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5ZEeenuV7Tvz"
   },
   "outputs": [],
   "source": [
    "# sentencepiece에서 32000개의 데이터를 추출하여 kor이라는 vocab모델을 만듦\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=구어체(1).txt --model_prefix=kor --vocab_size=32000 --model_type=bpe --max_sentence_length=9999')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vg7RVNTwHWMO"
   },
   "outputs": [],
   "source": [
    "# 복원을 위해 구어체(1).txt파일을 불러옴\n",
    "import pandas as pd\n",
    "\n",
    "word = pd.read_table('구어체(1).txt', names =['kor', 'eng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTBiZMRXF6Xu",
    "outputId": "f6df47c1-5e1c-4893-bc53-26bac04af6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 기존 한국어 문장: 'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다.\n",
      "1번째 sentencepiece로 추출해준 한국어 문장:  ▁' B ible ▁Col oring ' 은 ▁성경 의 ▁아름다운 ▁이야기를 ▁체험 ▁할 ▁수 ▁있는 ▁컬러 링 ▁앱입니다 .\n",
      "1번째 기존 영어 문장: Bible Coloring' is a coloring application that allows you to experience beautiful stories in the Bible.\n",
      "1번째 sentencepiece로 추출해준 한국어 문장:  ▁Bible ▁Col oring ' ▁is ▁a ▁coloring ▁application ▁that ▁allows ▁you ▁to ▁experience ▁beautiful ▁stories ▁in ▁the ▁Bible .\n",
      "\n",
      "2번째 기존 한국어 문장: 씨티은행에서 일하세요?\n",
      "2번째 sentencepiece로 추출해준 한국어 문장:  ▁씨 티 은행 에서 ▁일 하세요 ?\n",
      "2번째 기존 영어 문장: Do you work at a City bank?\n",
      "2번째 sentencepiece로 추출해준 한국어 문장:  ▁Do ▁you ▁work ▁at ▁a ▁City ▁bank ?\n",
      "\n",
      "3번째 기존 한국어 문장: 푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.\n",
      "3번째 sentencepiece로 추출해준 한국어 문장:  ▁푸 리토 의 ▁베스트 셀 러는 ▁해외에서 ▁입 소 문 만으로 ▁4 차 ▁완 판을 ▁기록 하였 다 .\n",
      "3번째 기존 영어 문장: PURITO's bestseller, which recorded 4th rough -cuts by words of mouth from abroad.\n",
      "3번째 sentencepiece로 추출해준 한국어 문장:  ▁PU R IT O ' s ▁best se ller , ▁which ▁recorded ▁4 th ▁rough ▁- cut s ▁by ▁words ▁of ▁mouth ▁from ▁abroad .\n",
      "\n",
      "4번째 기존 한국어 문장: 11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.\n",
      "4번째 sentencepiece로 추출해준 한국어 문장:  ▁11 장 에서는 ▁예수님 이 ▁이번엔 ▁나 사 로를 ▁무덤 에서 ▁불러 내 어 ▁죽은 ▁자 ▁가운데 서 ▁살 리 셨습니다 .\n",
      "4번째 기존 영어 문장: In Chapter 11 Jesus called Lazarus from the tomb and raised him from the dead.\n",
      "4번째 sentencepiece로 추출해준 한국어 문장:  ▁In ▁Ch ap ter ▁11 ▁Jesus ▁called ▁L az ar us ▁from ▁the ▁tomb ▁and ▁raised ▁him ▁from ▁the ▁dead .\n",
      "\n",
      "5번째 기존 한국어 문장: 6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.\n",
      "5번째 sentencepiece로 추출해준 한국어 문장:  ▁6 .5 , ▁7, ▁8 ▁사이즈가 ▁몇 ▁개나 ▁더 ▁재 입고 ▁될지 ▁제게 ▁알려주시면 ▁감사하겠습니다 .\n",
      "5번째 기존 영어 문장: I would feel grateful to know how many stocks will be secured of size 6.5, 7, and 8.\n",
      "5번째 sentencepiece로 추출해준 한국어 문장:  ▁I ▁would ▁feel ▁grateful ▁to ▁know ▁how ▁many ▁stocks ▁will ▁be ▁sec ured ▁of ▁size ▁6 .5 , ▁7, ▁and ▁8.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kor.model을 로드\n",
    "sp = spm.SentencePieceProcessor() \n",
    "vocab_file = \"kor.model\" \n",
    "sp.load(vocab_file)\n",
    "# sentencepiece로 복원할 문장 리스트 저장\n",
    "korean_lines= word['kor']\n",
    "english_lines = word['eng']\n",
    "\n",
    "# 문장 재생성(복원)\n",
    "kor_list = []\n",
    "eng_list = []\n",
    "for line in korean_lines:\n",
    "    new_kor = (sp.encode_as_pieces(line))\n",
    "    sentence = \"\"\n",
    "    for token in new_kor:\n",
    "      sentence+=(' '+token)\n",
    "    kor_list.append(sentence)\n",
    "    \n",
    "for line in english_lines:\n",
    "    new_kor = (sp.encode_as_pieces(line))\n",
    "    sentence = \"\"\n",
    "    for token in new_kor:\n",
    "      sentence+=(' '+token)\n",
    "    eng_list.append(sentence)\n",
    "\n",
    "# 문장이 재생성(복원)되었는지 확인 \n",
    "for i in range(0,5):\n",
    "  print(str(i+1)+\"번째 기존 한국어 문장:\", korean_lines[i])\n",
    "  print(str(i+1)+\"번째 sentencepiece로 추출해준 한국어 문장:\", kor_list[i])\n",
    "  print(str(i+1)+\"번째 기존 영어 문장:\", english_lines[i])\n",
    "\n",
    "  print(str(i+1)+\"번째 sentencepiece로 추출해준 한국어 문장:\", eng_list[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "48s9o8CiUNxB"
   },
   "outputs": [],
   "source": [
    "# 구어체(2).txt 파일로 저장\n",
    "f = open(\"구어체(2).txt\", \"w\")\n",
    "for i in range(len(kor_list)):\n",
    "    data = kor_list[i]+'\\t'+eng_list[i]+\"\\n\"\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "k9e4pWbBWcmZ",
    "outputId": "737b464d-0606-4f13-f8a2-5d2c7f8b2f3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kor</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁' B ible ▁Col oring ' 은 ▁성경 의 ▁아름다운 ▁이야기를 ▁체...</td>\n",
       "      <td>▁Bible ▁Col oring ' ▁is ▁a ▁coloring ▁applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁씨 티 은행 에서 ▁일 하세요 ?</td>\n",
       "      <td>▁Do ▁you ▁work ▁at ▁a ▁City ▁bank ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁푸 리토 의 ▁베스트 셀 러는 ▁해외에서 ▁입 소 문 만으로 ▁4 차 ▁완 판을...</td>\n",
       "      <td>▁PU R IT O ' s ▁best se ller , ▁which ▁record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁11 장 에서는 ▁예수님 이 ▁이번엔 ▁나 사 로를 ▁무덤 에서 ▁불러 내 어 ...</td>\n",
       "      <td>▁In ▁Ch ap ter ▁11 ▁Jesus ▁called ▁L az ar us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁6 .5 , ▁7, ▁8 ▁사이즈가 ▁몇 ▁개나 ▁더 ▁재 입고 ▁될지 ▁제게 ...</td>\n",
       "      <td>▁I ▁would ▁feel ▁grateful ▁to ▁know ▁how ▁man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 kor                                                eng\n",
       "0   ▁' B ible ▁Col oring ' 은 ▁성경 의 ▁아름다운 ▁이야기를 ▁체...   ▁Bible ▁Col oring ' ▁is ▁a ▁coloring ▁applica...\n",
       "1                                ▁씨 티 은행 에서 ▁일 하세요 ?                ▁Do ▁you ▁work ▁at ▁a ▁City ▁bank ?\n",
       "2   ▁푸 리토 의 ▁베스트 셀 러는 ▁해외에서 ▁입 소 문 만으로 ▁4 차 ▁완 판을...   ▁PU R IT O ' s ▁best se ller , ▁which ▁record...\n",
       "3   ▁11 장 에서는 ▁예수님 이 ▁이번엔 ▁나 사 로를 ▁무덤 에서 ▁불러 내 어 ...   ▁In ▁Ch ap ter ▁11 ▁Jesus ▁called ▁L az ar us...\n",
       "4   ▁6 .5 , ▁7, ▁8 ▁사이즈가 ▁몇 ▁개나 ▁더 ▁재 입고 ▁될지 ▁제게 ...   ▁I ▁would ▁feel ▁grateful ▁to ▁know ▁how ▁man..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구어체 파일 확인\n",
    "word2 = pd.read_table('구어체(2).txt', names =['kor', 'eng'])\n",
    "word2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_hV5r_VIVN-k"
   },
   "outputs": [],
   "source": [
    "# change path to bring file 구어체(2).txt\n",
    "path_to_file = \"/content/drive/MyDrive/natural_language_processing_6/구어체(2).txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHKSttW4T4Ry"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  #w = unicode_to_ascii(w.lower().strip())\n",
    "  w = w.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^0-9a-zA-Z가-힣?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opI2GzOt479E",
    "outputId": "53a28678-c5f4-4004-81b8-0b3a9526d945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTbSbBz55QtF",
    "outputId": "98d3a586-a693-467c-e361-f80c5f2aaf85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> when you make an order , he cooks on the steel pan right away . <end>\n",
      "<start> 네가 가서 주문하면 즉 석 에서 철 판에 요리를 해줘 . <end>\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "ko, en = create_dataset(path_to_file, None)\n",
    "print(en[80000])\n",
    "print(ko[80000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnxC7q-j3jFD",
    "outputId": "71794f67-043c-4a3f-aa3e-ede020231767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 100000\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "print(num_examples, len(input_tensor), len(target_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QILQkOs3jFG",
    "outputId": "6e95a898-8bdf-44e5-a431-156de7acaf21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXukARTDd7MT",
    "outputId": "bfe50084-e457-4185-b99b-645099e59a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "7 ----> you\n",
      "156 ----> didn\n",
      "20 ----> t\n",
      "138 ----> even\n",
      "60 ----> know\n",
      "62 ----> how\n",
      "7 ----> you\n",
      "121 ----> re\n",
      "394 ----> making\n",
      "23 ----> your\n",
      "355 ----> parents\n",
      "1460 ----> upset\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "44 ----> 네가\n",
      "377 ----> 얼마나\n",
      "77 ----> 네\n",
      "3546 ----> 부모\n",
      "53 ----> 를\n",
      "5251 ----> 화나게\n",
      "769 ----> 하는지\n",
      "6260 ----> 모르고\n",
      "2792 ----> 있었\n",
      "40 ----> 지\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wJ6UPiMu4w1",
    "outputId": "c176c745-bcd5-42e0-d737-2c88fcc5dd2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> <start>\n",
      "32 ----> if\n",
      "7 ----> you\n",
      "2507 ----> delete\n",
      "8 ----> a\n",
      "330 ----> game\n",
      "9 ----> ,\n",
      "5 ----> the\n",
      "498 ----> data\n",
      "25 ----> will\n",
      "39 ----> not\n",
      "29 ----> be\n",
      "3834 ----> recovered\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "1 ----> <start>\n",
      "811 ----> 게임\n",
      "2970 ----> 삭제\n",
      "94 ----> 시\n",
      "7314 ----> 데이터가\n",
      "12080 ----> 복구\n",
      "291 ----> 되지\n",
      "281 ----> 않습니다\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "convert(inp_lang, input_tensor_train[1000])\n",
    "\n",
    "convert(targ_lang, target_tensor_train[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqHsArVZ3jFS",
    "outputId": "368ef8d9-1a59-47a4-8703-4bd4b4eab1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9808 19225\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(vocab_inp_size, vocab_tar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "9f321715-5d2e-40a0-e3fe-994b386636ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 59]), TensorShape([64, 48]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from [Luong's paper](https://arxiv.org/abs/1508.04025v5). \n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "This tutorial uses [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) for the encoder. Let's decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "\n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60gSVh05Jl6l",
    "outputId": "ec609e47-56dd-4b83-eef3-b2b1dd5955d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 59, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  2510848   \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,449,152\n",
      "Trainable params: 6,449,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k534zTHiDjQU",
    "outputId": "654b11ba-b049-4dd2-b97c-944cbd16c736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 59, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5UY8wko3jFp",
    "outputId": "2c4394e0-b63f-44db-87ad-1fa430ea09f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 19225)\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  4921600   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  7084032   \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  19705625  \n",
      "                                                                 \n",
      " bahdanau_attention_1 (Bahda  multiple                 2100225   \n",
      " nauAttention)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,811,482\n",
      "Trainable params: 33,811,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddefjBMa3jF0",
    "outputId": "92d3ef8e-6ce7-42e8-b528-8b3e6e2e3ecd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.4563\n",
      "Epoch 1 Batch 100 Loss 1.7138\n",
      "Epoch 1 Batch 200 Loss 1.6240\n",
      "Epoch 1 Batch 300 Loss 1.5987\n",
      "Epoch 1 Batch 400 Loss 1.4784\n",
      "Epoch 1 Batch 500 Loss 1.6224\n",
      "Epoch 1 Batch 600 Loss 1.4535\n",
      "Epoch 1 Batch 700 Loss 1.5912\n",
      "Epoch 1 Batch 800 Loss 1.5985\n",
      "Epoch 1 Batch 900 Loss 1.7075\n",
      "Epoch 1 Batch 1000 Loss 1.5046\n",
      "Epoch 1 Batch 1100 Loss 1.4778\n",
      "Epoch 1 Batch 1200 Loss 1.5504\n",
      "Epoch 1 Loss 1.5675\n",
      "Time taken for 1 epoch 4184.392882108688 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.4908\n",
      "Epoch 2 Batch 100 Loss 1.3797\n",
      "Epoch 2 Batch 200 Loss 1.4656\n",
      "Epoch 2 Batch 300 Loss 1.2874\n",
      "Epoch 2 Batch 400 Loss 1.3237\n",
      "Epoch 2 Batch 500 Loss 1.3653\n",
      "Epoch 2 Batch 600 Loss 1.2564\n",
      "Epoch 2 Batch 700 Loss 1.1361\n",
      "Epoch 2 Batch 800 Loss 1.2596\n",
      "Epoch 2 Batch 900 Loss 1.2468\n",
      "Epoch 2 Batch 1000 Loss 1.2083\n",
      "Epoch 2 Batch 1100 Loss 1.1466\n",
      "Epoch 2 Batch 1200 Loss 1.1379\n",
      "Epoch 2 Loss 1.2477\n",
      "Time taken for 1 epoch 4122.5248067379 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.9752\n",
      "Epoch 3 Batch 100 Loss 0.9559\n",
      "Epoch 3 Batch 200 Loss 1.0462\n",
      "Epoch 3 Batch 300 Loss 1.2266\n",
      "Epoch 3 Batch 400 Loss 1.1635\n",
      "Epoch 3 Batch 500 Loss 0.9941\n",
      "Epoch 3 Batch 600 Loss 0.9688\n",
      "Epoch 3 Batch 700 Loss 1.2160\n",
      "Epoch 3 Batch 800 Loss 0.9566\n",
      "Epoch 3 Batch 900 Loss 0.9879\n",
      "Epoch 3 Batch 1000 Loss 0.8909\n",
      "Epoch 3 Batch 1100 Loss 0.9787\n",
      "Epoch 3 Batch 1200 Loss 1.0194\n",
      "Epoch 3 Loss 1.0395\n",
      "Time taken for 1 epoch 4183.7861704826355 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.7799\n",
      "Epoch 4 Batch 100 Loss 0.8491\n",
      "Epoch 4 Batch 200 Loss 0.8347\n",
      "Epoch 4 Batch 300 Loss 0.8848\n",
      "Epoch 4 Batch 400 Loss 1.0600\n",
      "Epoch 4 Batch 500 Loss 0.9332\n",
      "Epoch 4 Batch 600 Loss 0.8356\n",
      "Epoch 4 Batch 700 Loss 1.0439\n",
      "Epoch 4 Batch 800 Loss 0.8279\n",
      "Epoch 4 Batch 900 Loss 0.9100\n",
      "Epoch 4 Batch 1000 Loss 0.9122\n",
      "Epoch 4 Batch 1100 Loss 0.7520\n",
      "Epoch 4 Batch 1200 Loss 0.8020\n",
      "Epoch 4 Loss 0.8819\n",
      "Time taken for 1 epoch 4424.197720527649 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.7413\n",
      "Epoch 5 Batch 100 Loss 0.6228\n",
      "Epoch 5 Batch 200 Loss 0.7896\n",
      "Epoch 5 Batch 300 Loss 0.8331\n",
      "Epoch 5 Batch 400 Loss 0.7169\n",
      "Epoch 5 Batch 500 Loss 0.7569\n",
      "Epoch 5 Batch 600 Loss 0.6971\n",
      "Epoch 5 Batch 700 Loss 0.6630\n",
      "Epoch 5 Batch 800 Loss 0.8895\n",
      "Epoch 5 Batch 900 Loss 0.9095\n",
      "Epoch 5 Batch 1000 Loss 0.7900\n",
      "Epoch 5 Batch 1100 Loss 0.5835\n",
      "Epoch 5 Batch 1200 Loss 0.7140\n",
      "Epoch 5 Loss 0.7502\n",
      "Time taken for 1 epoch 4490.798162698746 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5979\n",
      "Epoch 6 Batch 100 Loss 0.5584\n",
      "Epoch 6 Batch 200 Loss 0.5719\n",
      "Epoch 6 Batch 300 Loss 0.5721\n",
      "Epoch 6 Batch 400 Loss 0.6567\n",
      "Epoch 6 Batch 500 Loss 0.6017\n",
      "Epoch 6 Batch 600 Loss 0.6776\n",
      "Epoch 6 Batch 700 Loss 0.6278\n",
      "Epoch 6 Batch 800 Loss 0.7968\n",
      "Epoch 6 Batch 900 Loss 0.6505\n",
      "Epoch 6 Batch 1000 Loss 0.6817\n",
      "Epoch 6 Batch 1100 Loss 0.6913\n",
      "Epoch 6 Batch 1200 Loss 0.5838\n",
      "Epoch 6 Loss 0.6319\n",
      "Time taken for 1 epoch 4601.851526260376 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.4653\n",
      "Epoch 7 Batch 100 Loss 0.4597\n",
      "Epoch 7 Batch 200 Loss 0.4508\n",
      "Epoch 7 Batch 300 Loss 0.4456\n",
      "Epoch 7 Batch 400 Loss 0.5499\n",
      "Epoch 7 Batch 500 Loss 0.5375\n",
      "Epoch 7 Batch 600 Loss 0.4869\n",
      "Epoch 7 Batch 700 Loss 0.5742\n",
      "Epoch 7 Batch 800 Loss 0.5515\n",
      "Epoch 7 Batch 900 Loss 0.6230\n",
      "Epoch 7 Batch 1000 Loss 0.5473\n",
      "Epoch 7 Batch 1100 Loss 0.5577\n",
      "Epoch 7 Batch 1200 Loss 0.6219\n",
      "Epoch 7 Loss 0.5268\n",
      "Time taken for 1 epoch 4237.991766214371 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.4216\n",
      "Epoch 8 Batch 100 Loss 0.4162\n",
      "Epoch 8 Batch 200 Loss 0.4278\n",
      "Epoch 8 Batch 300 Loss 0.4707\n",
      "Epoch 8 Batch 400 Loss 0.4222\n",
      "Epoch 8 Batch 500 Loss 0.4107\n",
      "Epoch 8 Batch 600 Loss 0.4786\n",
      "Epoch 8 Batch 700 Loss 0.4030\n",
      "Epoch 8 Batch 800 Loss 0.4781\n",
      "Epoch 8 Batch 900 Loss 0.4960\n",
      "Epoch 8 Batch 1000 Loss 0.4096\n",
      "Epoch 8 Batch 1100 Loss 0.4416\n",
      "Epoch 8 Batch 1200 Loss 0.4617\n",
      "Epoch 8 Loss 0.4351\n",
      "Time taken for 1 epoch 4208.704157114029 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.3158\n",
      "Epoch 9 Batch 100 Loss 0.3307\n",
      "Epoch 9 Batch 200 Loss 0.3410\n",
      "Epoch 9 Batch 300 Loss 0.3526\n",
      "Epoch 9 Batch 400 Loss 0.2655\n",
      "Epoch 9 Batch 500 Loss 0.3343\n",
      "Epoch 9 Batch 600 Loss 0.3324\n",
      "Epoch 9 Batch 700 Loss 0.3789\n",
      "Epoch 9 Batch 800 Loss 0.4095\n",
      "Epoch 9 Batch 900 Loss 0.3227\n",
      "Epoch 9 Batch 1000 Loss 0.3811\n",
      "Epoch 9 Batch 1100 Loss 0.3617\n",
      "Epoch 9 Batch 1200 Loss 0.4733\n",
      "Epoch 9 Loss 0.3580\n",
      "Time taken for 1 epoch 4246.672966957092 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2862\n",
      "Epoch 10 Batch 100 Loss 0.2564\n",
      "Epoch 10 Batch 200 Loss 0.2729\n",
      "Epoch 10 Batch 300 Loss 0.2603\n",
      "Epoch 10 Batch 400 Loss 0.3087\n",
      "Epoch 10 Batch 500 Loss 0.2369\n",
      "Epoch 10 Batch 600 Loss 0.3075\n",
      "Epoch 10 Batch 700 Loss 0.3210\n",
      "Epoch 10 Batch 800 Loss 0.3516\n",
      "Epoch 10 Batch 900 Loss 0.3046\n",
      "Epoch 10 Batch 1000 Loss 0.2832\n",
      "Epoch 10 Batch 1100 Loss 0.2713\n",
      "Epoch 10 Batch 1200 Loss 0.3476\n",
      "Epoch 10 Loss 0.2938\n",
      "Time taken for 1 epoch 4313.283221721649 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJpT9D5_OgP6",
    "outputId": "963ecfcd-c712-4c51-a21d-ec55266e9f5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa22be75f90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WrAM0FDomq3E",
    "outputId": "259075ab-1524-4dee-fefa-295f0b3ad1f0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> it is hard to save time <end>\n",
      "Predicted translation: 그것은 시간을 절약 하는 것은 어렵습니다 . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44536 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44163 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51008 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51012 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51208 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50557 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47157 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49845 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44536 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44163 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51008 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51012 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51208 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50557 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47157 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49845 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('it is hard to save time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zSx2iM36EZQZ",
    "outputId": "5f8f2d41-0878-4e0c-ab0f-760f7dcab421",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> you are pretty small <end>\n",
      "Predicted translation: 당신은 꽤 엄청나게 작습니다 . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45817 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49888 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51008 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44900 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50628 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52397 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45208 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44172 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51089 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49845 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45817 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49888 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51008 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44900 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50628 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52397 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45208 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44172 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51089 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49845 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('you are pretty small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A3LLCx3ZE0Ls",
    "outputId": "e3395ef1-b52b-494b-90ea-264e69b6c073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> why does he have long hair ? <end>\n",
      "Predicted translation: 그는 왜 는 왜 이렇게 멀 나요 ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44536 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50780 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47111 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44172 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47680 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45208 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50836 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44536 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50780 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47111 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44172 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47680 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45208 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50836 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('why does he have long hair?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DUQVLVqUE1YW",
    "outputId": "c55de086-da1a-4bdc-fe6c-3498fc952d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> when do we take dinner ? <end>\n",
      "Predicted translation: 너는 언제 사 니 ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45320 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50616 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51228 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45320 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50616 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51228 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wrong translation\n",
    "translate('when do we take dinner?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "kor_eng.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
