{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e494d1",
   "metadata": {},
   "source": [
    "# Word-Level 번역기 만들기(Neural Machine Translation (seq2seq) Tutorial)\n",
    "\n",
    "- 문제 1번 : 15장 수업에서 다룬 프로그램은 교재의 15.2절에서 찾을 수 있다. 교재에 나온 순서대로 프로그램을 구현하여 수업 자료에서 제시한 것과 유사하게 10개의 샘플을 수행하여 결과를 제시하라. 10개의 샘플은 자신의 학번 마지막 2자리를 이용하여 생성한 숫자를 이용하여 선택한다. 예를 들면 자신의 학번이 2018003371 이면 [171, 271, …,1071] 등 10개의 숫자를 제시한다. \n",
    "  - 답 : 다음과 같이 학번 뒤의 65를 이용해서 [165, 265, ... , 1065]를 생성하여 15.2절의 프로그램의 코딩을 진행하였고, 아래와 같은 내용일 출력되었습니다. 자세한 코드 실행은 아래 문제 1:코드에서 확인하시면 됩니다.!\n",
    "  - 리스트에 학번을 통한 인덱스 생성 뒤 저장 : <img src=\"./list.png\" width=450 height=300 alt=\"잘못된주소\"></img>\n",
    "  - 훈련 데이터 샘플 : <img src=\"./output1.png\" width=350 height=300 alt=\"잘못된주소\"></img>\n",
    "  - 테스트 데이터 샘플: <img src=\"./output2.png\" width=300 height=300 alt=\"잘못된주소\"></img>\n",
    "- 문제 2번 : 훈련이 끝난 시스템을 나중에 다시 사용할 수 있게 저장하는 방법을 찾아낸다. 이 시스템의 경우 어떻게 저장되는지 설명하라.\n",
    "  - 답 : model.save('model_name.h5')로 모델을 hdf5파일에 저장해줍니다.hdf5파일은 계층적인 구조를 가지며 hdf파일을 통해서 모델의 정확도 및 loss를 저장해주고 사용해줄 수 있습니다. 파일-디렉토리 형태로 트리구조를 가지며, 대용량으로 저장되어있으며 이 후`from tensorflow.keras.models import load_model`와 같이 tensorflow.keras.model 패키지에 있는 load_model함수를 통해 model을 불러와 재활용해줄 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f1775",
   "metadata": {},
   "source": [
    "### 문제 1 : \n",
    "- 15장 수업에서 다룬 프로그램은 교재의 15.2절에서 찾을 수 있다. 교재에 나온 순서대로 프로그램을 구현하여 수업 자료에서 제시한 것과 유사하게 10개의 샘플을 수행하여 결과를 제시하라. 10개의 샘플은 자신의 학번 마지막 2자리를 이용하여 생성한 숫자를 이용하여 선택한다. 예를 들면 자신의 학번이 2018003371 이면 [171, 271, …,1071] 등 10개의 숫자를 제시한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ba7f7",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리\n",
    "- 필요한 도구들을 import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a588c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e1eee",
   "metadata": {},
   "source": [
    "#### 데이터를 로드\n",
    "- fra.txt 데이터 구조 : 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 구조."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb84bc",
   "metadata": {},
   "source": [
    "- 이번 챕터에서는 총 33,000개의 샘플을 사용할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67fef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdfc9c",
   "metadata": {},
   "source": [
    "- 전처리 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c49448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8bd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    # 위에서 구현한 함수를 내부적으로 호출\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c78f39",
   "metadata": {},
   "source": [
    "- 구현한 전처리 함수들을 임의의 문장을 입력으로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bc0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have you had dinner ?\n",
      "b'avez vous deja dine ?'\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "print(preprocess_sentence(en_sent))\n",
    "print(preprocess_sentence(fr_sent).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0b427",
   "metadata": {},
   "source": [
    "#### 모든 전처리를 수행하는 함수 구현\n",
    "- 전체 데이터에서 33,000개의 샘플불러옴. \n",
    "- 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정=> 훈련 시 사용할 디코더의 입력 시퀀스와 실제값에 해당되는 출력 시퀀스를 따로 분리하여 저장\n",
    "- 입력 시퀀스에는 시작을 의미하는 토큰인 `<sos>`를 추가 \n",
    "- 출력 시퀀스에는 종료를 의미하는 토큰인`<eos>`를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b0d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"fra.txt\", \"r\", encoding=\"utf-8\") as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line_input = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_input = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_target = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line_input)\n",
    "            decoder_input.append(tar_line_input)\n",
    "            decoder_target.append(tar_line_target)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60c912",
   "metadata": {},
   "source": [
    "- 인코더의 입력, 디코더의 입력, 디코더의 실제값을 상위 5개 샘플만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbd3594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
      "[['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
      "[['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print(sents_en_in[:5])\n",
    "print(sents_fra_in[:5])\n",
    "print(sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f49a8",
   "metadata": {},
   "source": [
    "- 케라스 토크나이저를 통해 단어 집합을 생성하고, 텍스트 시퀀스를 정수 시퀀스로 변환하는 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832fbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb4368",
   "metadata": {},
   "source": [
    "- 패딩을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a015db",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63948c",
   "metadata": {},
   "source": [
    "- 데이터의 크기(shape)를 확인\n",
    "- 샘플은 총 33,000개 존재하며 영어 문장의 길이는 8, 프랑스어 문장의 길이 16\n",
    "- 단어 집합의 크기를 정의\n",
    "- 단어 집합의 크기는 각각 4,647개와 8,022개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f28b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4606, 프랑스어 단어 집합의 크기 : 8107\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ba3d6",
   "metadata": {},
   "source": [
    "- 단어로부터 정수를 얻는 딕셔너리 구현\n",
    "- 정수로부터 단어를 얻는 딕셔너리 구현 \n",
    "  - 훈련을 마치고 예측 과정과 실제값과 결과를 비교하는 경우에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e518d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word # 훈련 후 결과 비교할 때 사용\n",
    "\n",
    "tar_to_index = tokenizer_fra.word_index # 훈련 후 예측 과정에서 사용\n",
    "index_to_tar = tokenizer_fra.index_word # 훈련 후 결과 비교할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c92390",
   "metadata": {},
   "source": [
    "- 테스트 데이터를 분리하기 전에, 적절한 분포를 갖도록 데이터를 섞어주는 과정을 진행\n",
    "  - 이를 위해서 우선 순서가 섞인 정수 시퀀스 리스트 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "904596b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1660 19852 27911 ...  6246  9413  8224]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ac510",
   "metadata": {},
   "source": [
    "- 데이터셋의 순서로 지정, 샘플들이 기존 순서와 다른 순서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d7d6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a26ac6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1984,    3,  136,    1,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f79112e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  122,   19, 6795,    1,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c69de677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 122,   19, 6795,    1,    3,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target[30997]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b95c4",
   "metadata": {},
   "source": [
    "- 테스트 데이터 : 33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "553e2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print(n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f4c8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8d477",
   "metadata": {},
   "source": [
    "- 훈련 데이터와 테스트 데이터의 크기(shape)를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7a090ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29700, 8)\n",
      "(29700, 16)\n",
      "(29700, 16)\n",
      "(3300, 8)\n",
      "(3300, 16)\n",
      "(3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b47f53",
   "metadata": {},
   "source": [
    "## 2. 기계번역어 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32ad63",
   "metadata": {},
   "source": [
    "- 필요한 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0fbb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a187ec0",
   "metadata": {},
   "source": [
    "- 임베딩 벡터, LSTM의 은닉 상태의 크기를 50으로 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e35ffb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09eaa9",
   "metadata": {},
   "source": [
    "- 인코더 설계\n",
    "  - Masking은 패딩 토큰인 숫자 0의 경우에는 연산을 제외하는 역할을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c3ad759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9447cae",
   "metadata": {},
   "source": [
    "- 디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "654a07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b46ee",
   "metadata": {},
   "source": [
    "- 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7900f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d5850a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960219c3",
   "metadata": {},
   "source": [
    "- 모델의 파라미터를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d67a71cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     230300      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     405350      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 50)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 50)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50), (None,  20200       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 50), ( 20200       masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8107)   413457      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,089,507\n",
      "Trainable params: 1,089,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e120066",
   "metadata": {},
   "source": [
    "- 모델을 훈련: \n",
    "    - 128개의 배치 크기로 총 50 에포크 학습 \n",
    "    - 테스트 데이터를 검증 데이터로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "970b94b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/233 [==============================] - 106s 420ms/step - loss: 3.1240 - acc: 0.6198 - val_loss: 1.8339 - val_acc: 0.7122\n",
      "Epoch 2/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 1.6721 - acc: 0.7386 - val_loss: 1.5506 - val_acc: 0.7534\n",
      "Epoch 3/50\n",
      "233/233 [==============================] - 94s 406ms/step - loss: 1.4931 - acc: 0.7575 - val_loss: 1.4464 - val_acc: 0.7663\n",
      "Epoch 4/50\n",
      "233/233 [==============================] - 91s 391ms/step - loss: 1.3921 - acc: 0.7723 - val_loss: 1.3818 - val_acc: 0.7805\n",
      "Epoch 5/50\n",
      "233/233 [==============================] - 92s 395ms/step - loss: 1.3088 - acc: 0.7886 - val_loss: 1.3012 - val_acc: 0.7934\n",
      "Epoch 6/50\n",
      "233/233 [==============================] - 94s 404ms/step - loss: 1.2390 - acc: 0.7992 - val_loss: 1.2472 - val_acc: 0.8008\n",
      "Epoch 7/50\n",
      "233/233 [==============================] - 93s 400ms/step - loss: 1.1840 - acc: 0.8073 - val_loss: 1.2160 - val_acc: 0.8041\n",
      "Epoch 8/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 1.1410 - acc: 0.8131 - val_loss: 1.1679 - val_acc: 0.8130\n",
      "Epoch 9/50\n",
      "233/233 [==============================] - 93s 401ms/step - loss: 1.1042 - acc: 0.8190 - val_loss: 1.1231 - val_acc: 0.8202\n",
      "Epoch 10/50\n",
      "233/233 [==============================] - 93s 399ms/step - loss: 1.0697 - acc: 0.8238 - val_loss: 1.1046 - val_acc: 0.8221\n",
      "Epoch 11/50\n",
      "233/233 [==============================] - 94s 401ms/step - loss: 1.0386 - acc: 0.8280 - val_loss: 1.0714 - val_acc: 0.8274\n",
      "Epoch 12/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 1.0118 - acc: 0.8318 - val_loss: 1.0687 - val_acc: 0.8281\n",
      "Epoch 13/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.9880 - acc: 0.8348 - val_loss: 1.0495 - val_acc: 0.8307\n",
      "Epoch 14/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.9670 - acc: 0.8375 - val_loss: 1.0222 - val_acc: 0.8347\n",
      "Epoch 15/50\n",
      "233/233 [==============================] - 94s 404ms/step - loss: 0.9477 - acc: 0.8402 - val_loss: 1.0264 - val_acc: 0.8333\n",
      "Epoch 16/50\n",
      "233/233 [==============================] - 94s 405ms/step - loss: 0.9301 - acc: 0.8427 - val_loss: 1.0060 - val_acc: 0.8364\n",
      "Epoch 17/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.9139 - acc: 0.8448 - val_loss: 0.9937 - val_acc: 0.8386\n",
      "Epoch 18/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.8984 - acc: 0.8470 - val_loss: 0.9786 - val_acc: 0.8399\n",
      "Epoch 19/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.8838 - acc: 0.8492 - val_loss: 0.9705 - val_acc: 0.8417\n",
      "Epoch 20/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.8700 - acc: 0.8510 - val_loss: 0.9722 - val_acc: 0.8411\n",
      "Epoch 21/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.8569 - acc: 0.8529 - val_loss: 0.9450 - val_acc: 0.8449\n",
      "Epoch 22/50\n",
      "233/233 [==============================] - 94s 404ms/step - loss: 0.8439 - acc: 0.8548 - val_loss: 0.9396 - val_acc: 0.8457\n",
      "Epoch 23/50\n",
      "233/233 [==============================] - 95s 406ms/step - loss: 0.8321 - acc: 0.8565 - val_loss: 0.9322 - val_acc: 0.8476\n",
      "Epoch 24/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.8205 - acc: 0.8584 - val_loss: 0.9294 - val_acc: 0.8463\n",
      "Epoch 25/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.8096 - acc: 0.8597 - val_loss: 0.9252 - val_acc: 0.8483\n",
      "Epoch 26/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.7990 - acc: 0.8612 - val_loss: 0.9155 - val_acc: 0.8496\n",
      "Epoch 27/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.7886 - acc: 0.8628 - val_loss: 0.9058 - val_acc: 0.8513\n",
      "Epoch 28/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.7789 - acc: 0.8643 - val_loss: 0.9061 - val_acc: 0.8500\n",
      "Epoch 29/50\n",
      "233/233 [==============================] - 95s 409ms/step - loss: 0.7702 - acc: 0.8658 - val_loss: 0.8991 - val_acc: 0.8518\n",
      "Epoch 30/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.7611 - acc: 0.8674 - val_loss: 0.8983 - val_acc: 0.8515\n",
      "Epoch 31/50\n",
      "233/233 [==============================] - 94s 404ms/step - loss: 0.7519 - acc: 0.8687 - val_loss: 0.8866 - val_acc: 0.8530\n",
      "Epoch 32/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.7434 - acc: 0.8702 - val_loss: 0.9011 - val_acc: 0.8524\n",
      "Epoch 33/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.7354 - acc: 0.8716 - val_loss: 0.8892 - val_acc: 0.8520\n",
      "Epoch 34/50\n",
      "233/233 [==============================] - 94s 404ms/step - loss: 0.7275 - acc: 0.8728 - val_loss: 0.8741 - val_acc: 0.8550\n",
      "Epoch 35/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.7206 - acc: 0.8739 - val_loss: 0.8814 - val_acc: 0.8541\n",
      "Epoch 36/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.7143 - acc: 0.8754 - val_loss: 0.8655 - val_acc: 0.8566\n",
      "Epoch 37/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.7083 - acc: 0.8767 - val_loss: 0.8707 - val_acc: 0.8573\n",
      "Epoch 38/50\n",
      "233/233 [==============================] - 94s 401ms/step - loss: 0.7027 - acc: 0.8777 - val_loss: 0.8660 - val_acc: 0.8563\n",
      "Epoch 39/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6969 - acc: 0.8789 - val_loss: 0.8618 - val_acc: 0.8579\n",
      "Epoch 40/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6919 - acc: 0.8801 - val_loss: 0.8594 - val_acc: 0.8589\n",
      "Epoch 41/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.6867 - acc: 0.8812 - val_loss: 0.8601 - val_acc: 0.8589\n",
      "Epoch 42/50\n",
      "233/233 [==============================] - 94s 401ms/step - loss: 0.6818 - acc: 0.8824 - val_loss: 0.8619 - val_acc: 0.8584\n",
      "Epoch 43/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6768 - acc: 0.8833 - val_loss: 0.8624 - val_acc: 0.8588\n",
      "Epoch 44/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6720 - acc: 0.8842 - val_loss: 0.8562 - val_acc: 0.8591\n",
      "Epoch 45/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6671 - acc: 0.8853 - val_loss: 0.8564 - val_acc: 0.8606\n",
      "Epoch 46/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6624 - acc: 0.8863 - val_loss: 0.8497 - val_acc: 0.8616\n",
      "Epoch 47/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6575 - acc: 0.8872 - val_loss: 0.8530 - val_acc: 0.8606\n",
      "Epoch 48/50\n",
      "233/233 [==============================] - 94s 402ms/step - loss: 0.6531 - acc: 0.8880 - val_loss: 0.8484 - val_acc: 0.8614\n",
      "Epoch 49/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.6483 - acc: 0.8890 - val_loss: 0.8435 - val_acc: 0.8630\n",
      "Epoch 50/50\n",
      "233/233 [==============================] - 94s 403ms/step - loss: 0.6439 - acc: 0.8899 - val_loss: 0.8476 - val_acc: 0.8612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d819aaa580>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6143f",
   "metadata": {},
   "source": [
    "## 3. seq2seq 기계 번역기 동작시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9277afa",
   "metadata": {},
   "source": [
    "- seq2seq는 훈련 과정과 테스트 과정에서의 동작 방식이 다름\n",
    "  -  따라서 테스트 과정을 위해 모델 재설계\n",
    "  -  인코더 / 디코더 재설계 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e16da77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "587fe259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d78e96",
   "metadata": {},
   "source": [
    "- 디코더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdfee78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e847a2e",
   "metadata": {},
   "source": [
    "- 테스트 과정에서의 동작을 위한 decode_sequence 함수 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df79f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909f0c5",
   "metadata": {},
   "source": [
    "- 결과 확인을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2977dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    sentence = ''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            sentence = sentence + index_to_src[i]+' '\n",
    "    return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    sentence =''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
    "            sentence = sentence + index_to_tar[i] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251600b8",
   "metadata": {},
   "source": [
    "#### 학번 맨 뒤 2자리를 이용한 인덱스 리스트 생성\n",
    "- **********65가 학번이므로 65만이용해서 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42b593f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[165, 265, 365, 465, 565, 665, 765, 865, 965, 1065]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudentID = []\n",
    "for i in range(1,11):\n",
    "    StudentID.append(int(str(i)+\"65\"))\n",
    "StudentID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995f1e7",
   "metadata": {},
   "source": [
    "- 훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd89efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  tom gargled . \n",
      "번역문 : tom s est gargarise . \n",
      "예측문 : tom s est assis . \n",
      "\n",
      "\n",
      "원문 :  be kind to others . \n",
      "번역문 : soyez aimable avec autrui ! \n",
      "예측문 : sois gentil avec elle ! \n",
      "\n",
      "\n",
      "원문 :  you can rely on me . \n",
      "번역문 : tu peux me faire confiance . \n",
      "예측문 : tu peux me faire confiance . \n",
      "\n",
      "\n",
      "원문 :  they re all lying . \n",
      "번역문 : ils mentent tous . \n",
      "예측문 : ils ont ete en train de partir . \n",
      "\n",
      "\n",
      "원문 :  they are actors . \n",
      "번역문 : ils sont acteurs . \n",
      "예측문 : ils sont des retour . \n",
      "\n",
      "\n",
      "원문 :  get out of my way . \n",
      "번역문 : ecarte toi de mon chemin . \n",
      "예측문 : sors de mon chemin ! \n",
      "\n",
      "\n",
      "원문 :  terrific ! \n",
      "번역문 : formidable ! \n",
      "예측문 : bien ! \n",
      "\n",
      "\n",
      "원문 :  i just got a job . \n",
      "번역문 : je viens d avoir un boulot . \n",
      "예측문 : je viens de mon travail . \n",
      "\n",
      "\n",
      "원문 :  i ll pack . \n",
      "번역문 : je ferai mon sac . \n",
      "예측문 : je vais me connais . \n",
      "\n",
      "\n",
      "원문 :  that s no accident . \n",
      "번역문 : ce n est pas un hasard . \n",
      "예측문 : ce n est pas un probleme . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in StudentID:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[1:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9122c2f",
   "metadata": {},
   "source": [
    "- 테스트 데이터에서 임의로 선택한 인덱스의 샘플의 결과출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c957c080",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  i m fearless . \n",
      "번역문 : je suis intrepide . \n",
      "예측문 : je suis me sens . \n",
      "\n",
      "\n",
      "원문 :  that always helps . \n",
      "번역문 : ca ne peut pas faire de mal . \n",
      "예측문 : ca ne m a pas parler . \n",
      "\n",
      "\n",
      "원문 :  i didn t argue . \n",
      "번역문 : je ne me suis pas disputee . \n",
      "예측문 : je suis vraiment . \n",
      "\n",
      "\n",
      "원문 :  this is an apple . \n",
      "번역문 : ceci est une pomme . \n",
      "예측문 : c est une en . \n",
      "\n",
      "\n",
      "원문 :  you re so mean . \n",
      "번역문 : vous etes si mechantes . \n",
      "예측문 : vous etes si serieux . \n",
      "\n",
      "\n",
      "원문 :  we were bored . \n",
      "번역문 : nous nous ennuyions . \n",
      "예측문 : nous nous avons des eau . \n",
      "\n",
      "\n",
      "원문 :  you re nuts ! \n",
      "번역문 : t es givre ! \n",
      "예측문 : vous etes vous ta sens . \n",
      "\n",
      "\n",
      "원문 :  you re very rude . \n",
      "번역문 : vous etes fort grossiere . \n",
      "예측문 : vous etes fort contrariee . \n",
      "\n",
      "\n",
      "원문 :  i do love you . \n",
      "번역문 : je t aime ! \n",
      "예측문 : je l aime toi . \n",
      "\n",
      "\n",
      "원문 :  this seems fair . \n",
      "번역문 : cela me semble honnete . \n",
      "예측문 : ca semble bon . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in StudentID:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_test[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_test[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[1:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaadb9a",
   "metadata": {},
   "source": [
    "### 문제 2번 : \n",
    "- 훈련이 끝난 시스템을 나중에 다시 사용할 수 있게 저장하는 방법을 찾아낸다. 이 시스템의 경우 어떻게 저장되는지 설명하라.\n",
    "  - 답 : model.save(\"Word_level_Translator.h5\")로 모델을 hdf5파일에 저장해줍니다.hdf5파일은 계층적인 구조를 가지며 hdf파일을 통해서 모델의 정확도 및 loss를 저장해주고 사용해줄 수 있습니다. 파일-디렉토리 형태로 트리구조를 가지며, 대용량으로 저장되어있으며 이 후`from tensorflow.keras.models import load_model`와 같이 tensorflow.keras.model 패키지에 있는 load_model함수를 통해 model을 불러와 재활용해줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52f74627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 학습 모델 저장\n",
    "model.save(\"Word_level_translator.h5\")\n",
    "\n",
    "# 테스트를 하기 위한 재설계된 모델 저장\n",
    "encoder_model.save(\"encoder_model.h5\")\n",
    "decoder_model.save(\"decoder_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
