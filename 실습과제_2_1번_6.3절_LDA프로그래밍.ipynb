{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9521b93f",
   "metadata": {},
   "source": [
    "# 잠재 디리클레 할당(LDA) 실습2\n",
    "\n",
    "\n",
    "- 교재 6.3절에 있는 잠재디리클레 할당(LDA)실습 2의 실습내용을 수행한 코드 및 출력 결과입니다.\n",
    "- 1. 교재의 6.3절에 있는 LDA 분석 프로그램을 수행하여 최종 결과를 얻는 문제의 답안입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09693c4",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽어오기\n",
    "\n",
    "- 링크 : https://www.kaggle.com/therohk/million-headlines\n",
    "- 약 15년간 발행했던 뉴스 기사 제목을 모아둔 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a571ded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082168\n"
     ]
    }
   ],
   "source": [
    "# 15년간 뉴스 기사 제목 데이터를 URL을 통해 받아옴\n",
    "# 링크 :  https://www.kaggle.com/therohk/million-headlines\n",
    "import pandas as pd \n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d681efb",
   "metadata": {},
   "source": [
    "## 2.  데이터 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de44f63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "# 5개 확인\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89fdba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    publish_date                                      headline_text\n",
      "0       20030219  aba decides against community broadcasting lic...\n",
      "1       20030219     act fire witnesses must be aware of defamation\n",
      "2       20030219     a g calls for infrastructure protection summit\n",
      "3       20030219           air nz staff in aust strike for pay rise\n",
      "4       20030219      air nz strike to affect australian travellers\n",
      "..           ...                                                ...\n",
      "95      20030219           meeting to consider tick clearance costs\n",
      "96      20030219         meeting to focus on broken hill water woes\n",
      "97      20030219                      moderate lift in wages growth\n",
      "98      20030219      more than 40 pc of young men drink alcohol at\n",
      "99      20030219  more water restrictions predicted for northern...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 100개 확인\n",
    "print(data.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c6d81",
   "metadata": {},
   "source": [
    "## 3. 텍스트 전처리\n",
    "\n",
    "- 영어 단어에 대한 불용어 제거, 표제어 추출, 길이가 짧은 단어 제거 등을 수행\n",
    "1. nltk의 word_tokenize를 실행하면 문장을 단어로 분리\n",
    "2. nltk의 stopwords를 이용하여 불용어를 제거\n",
    "3. nltk의 WordNetLemmatizer를 이용하여 동사 단어들을 정제(과거를 현재로, 3인칭을 1인칭으로)\n",
    "4. 단어 길이가 3이하인 단어들(aba, act, nz, air 등)을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1f918",
   "metadata": {},
   "source": [
    "### 3.1 텍스트 전처리 1\n",
    "\n",
    "- 속도가 느리나, 이런 경우 Google Colab을 활용하면 빠르게 문제를 처리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c134f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_6280/4206435398.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0   [aba, decides, community, broadcasting, licence]\n",
      "1    [act, fire, witnesses, must, aware, defamation]\n",
      "2     [g, calls, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_6280/4206435398.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])\n"
     ]
    }
   ],
   "source": [
    "# column이름만 추출\n",
    "text = data[['headline_text']]\n",
    "\n",
    "import nltk\n",
    "\n",
    "# word tokenization\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
    "\n",
    "# stop words removal 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46afdc0",
   "metadata": {},
   "source": [
    "### 3.2 텍스트 전처리 2\n",
    "\n",
    "- 길이가 작은 단어들 처리 (길이가 3이하의 단어들 제거 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "293d00a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_6280/2041771923.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [decide, community, broadcast, licence]\n",
      "1      [fire, witness, must, aware, defamation]\n",
      "2    [call, infrastructure, protection, summit]\n",
      "3                   [staff, aust, strike, rise]\n",
      "4      [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3]) \n",
    "\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c6ae9",
   "metadata": {},
   "source": [
    "## 4. tf-idf 행렬 만들기\n",
    "\n",
    "- sklearn 함수인 TfidfVectorizer는 입력 데이터로 단어 집합이 아니라 문장을 사용함: 정제된 단어들에 대해 detokenize를 수행하여 문장을 다시 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eebb6979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_6280/2844421375.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = detokenized_doc # 다시 text['headline_text']에 재저장\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 역토큰화 (토큰화 작업을 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc # 다시 text['headline_text']에 재저장\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 상위 1,000개의 단어를 보존\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000)\n",
    "\n",
    "X = vectorizer.fit_transform(text['headline_text']) \n",
    "# TF-IDF 행렬의 크기 확인 \n",
    "X.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbde0cf",
   "metadata": {},
   "source": [
    "## 5. 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65bf6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('government', 8725.19), ('sydney', 8393.29), ('queensland', 7720.12), ('change', 5874.27), ('home', 5674.38)]\n",
      "Topic 2: [('australia', 13691.08), ('australian', 11088.95), ('melbourne', 7528.43), ('world', 6707.7), ('south', 6677.03)]\n",
      "Topic 3: [('death', 5935.06), ('interview', 5924.98), ('kill', 5851.6), ('jail', 4632.85), ('life', 4275.27)]\n",
      "Topic 4: [('house', 6113.49), ('2016', 5488.19), ('state', 4923.41), ('brisbane', 4857.21), ('tasmania', 4610.97)]\n",
      "Topic 5: [('court', 7542.74), ('attack', 6959.64), ('open', 5663.0), ('face', 5193.63), ('warn', 5115.01)]\n",
      "Topic 6: [('market', 5545.86), ('rural', 5502.89), ('plan', 4828.71), ('indigenous', 4223.4), ('power', 3968.26)]\n",
      "Topic 7: [('charge', 8428.8), ('election', 7561.63), ('adelaide', 6758.36), ('make', 5658.99), ('test', 5062.69)]\n",
      "Topic 8: [('police', 12092.44), ('crash', 5281.14), ('drug', 4290.87), ('beat', 3257.58), ('rise', 2934.92)]\n",
      "Topic 9: [('fund', 4693.03), ('labor', 4047.69), ('national', 4038.68), ('council', 4006.62), ('claim', 3604.75)]\n",
      "Topic 10: [('trump', 11966.41), ('perth', 6456.53), ('report', 5611.33), ('school', 5465.06), ('woman', 5456.76)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1) \n",
    "lda_top=lda_model.fit_transform(X)\n",
    "\n",
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨. \n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "\tfor idx, topic in enumerate(components):\n",
    "\t\tprint(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "get_topics(lda_model.components_,terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
