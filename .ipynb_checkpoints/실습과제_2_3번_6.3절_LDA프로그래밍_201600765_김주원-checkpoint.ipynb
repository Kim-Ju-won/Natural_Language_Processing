{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab6fed6",
   "metadata": {},
   "source": [
    "# 잠재 디리클레 할당(LDA) 실습2\n",
    "\n",
    "- 교재 6.3절에 있는 잠재디리클레 할당(LDA)실습 2의 실습내용을 수행한 코드 및 출력 결과입니다.\n",
    "- 3. 이 자료 6쪽에 있는 처리 단어수를 3,000개와 5,000개로 변화시키면서 프로그램을 수행시키고 위의 2번 결과와 비교함문제의 답안입니다.\n",
    "\n",
    "- 수정사항 : \n",
    "1000개, 3000개, 5000개일 때 비교해주기 위해서 4.tf-idf에서 TfidfVectorizer함수의 max_features 수를 1000, 3000, 5000개로 변화시켜 변수에 저장하고 난뒤, 모델링 뒤 함수를 호출하여 각각에 해당하는 모델링 결과를 출력해주었습니다.\n",
    "\n",
    "\n",
    "- 답안 :\n",
    "\n",
    "  - 4.tf-idf 행렬만들기에서는 shape(모양)이\n",
    "    1000개일 때는 (1082168, 1000)\n",
    "    3000개일 때는 (1082168, 3000)\n",
    "    5000개일 때는 (1082168, 5000)\n",
    "    와 같이 행은 같으나, 열의 갯수가 max_features와 같이 증가하는 것을 확인할 수 있었습니다. \n",
    "    \n",
    "  - 5.토픽 모델링의 출력값은 같은 Topic 1이라도 max_features수가 달라지면 다른 단어를 출력하는 것을 확인할 수 있었습니다. 이는 단어의 분포가 달라져서 나오는 것이 주된 이유인데, 관련 부분은 맨 마지막페이지 아래 출력 값을 확인해주시면 됩니다.\n",
    "\n",
    "\n",
    "- 출력값 참고 : 아래 4.1~4.3에서는 1000개, 3000개, 5000개일 때 tf-idf 행렬 모양을, 그리고 5.토픽 모델링에Topic 단어들을 출력값을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b40d2",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽어오기\n",
    "\n",
    "- 링크 : https://www.kaggle.com/therohk/million-headlines\n",
    "- 약 15년간 발행했던 뉴스 기사 제목을 모아둔 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3b5310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082168\n"
     ]
    }
   ],
   "source": [
    "# 15년간 뉴스 기사 제목 데이터를 URL을 통해 받아옴\n",
    "# 링크 :  https://www.kaggle.com/therohk/million-headlines\n",
    "import pandas as pd \n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae52c5c",
   "metadata": {},
   "source": [
    "## 2.  데이터 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3492c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "# 5개 확인\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285c62f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    publish_date                                      headline_text\n",
      "0       20030219  aba decides against community broadcasting lic...\n",
      "1       20030219     act fire witnesses must be aware of defamation\n",
      "2       20030219     a g calls for infrastructure protection summit\n",
      "3       20030219           air nz staff in aust strike for pay rise\n",
      "4       20030219      air nz strike to affect australian travellers\n",
      "..           ...                                                ...\n",
      "95      20030219           meeting to consider tick clearance costs\n",
      "96      20030219         meeting to focus on broken hill water woes\n",
      "97      20030219                      moderate lift in wages growth\n",
      "98      20030219      more than 40 pc of young men drink alcohol at\n",
      "99      20030219  more water restrictions predicted for northern...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 100개 확인\n",
    "print(data.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae2be5",
   "metadata": {},
   "source": [
    "## 3. 텍스트 전처리\n",
    "\n",
    "- 영어 단어에 대한 불용어 제거, 표제어 추출, 길이가 짧은 단어 제거 등을 수행\n",
    "1. nltk의 word_tokenize를 실행하면 문장을 단어로 분리\n",
    "2. nltk의 stopwords를 이용하여 불용어를 제거\n",
    "3. nltk의 WordNetLemmatizer를 이용하여 동사 단어들을 정제(과거를 현재로, 3인칭을 1인칭으로)\n",
    "4. 단어 길이가 3이하인 단어들(aba, act, nz, air 등)을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd7f8e",
   "metadata": {},
   "source": [
    "### 3.1 텍스트 전처리 1\n",
    "\n",
    "- 속도가 느리나, 이런 경우 Google Colab을 활용하면 빠르게 문제를 처리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c60597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_9632/4206435398.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0   [aba, decides, community, broadcasting, licence]\n",
      "1    [act, fire, witnesses, must, aware, defamation]\n",
      "2     [g, calls, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_9632/4206435398.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])\n"
     ]
    }
   ],
   "source": [
    "# column이름만 추출\n",
    "text = data[['headline_text']]\n",
    "\n",
    "import nltk\n",
    "\n",
    "# word tokenization\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
    "\n",
    "# stop words removal 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2323fe",
   "metadata": {},
   "source": [
    "### 3.2 텍스트 전처리 2\n",
    "\n",
    "- 길이가 작은 단어들 처리 (길이가 3이하의 단어들 제거 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e5f3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_9632/2041771923.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [decide, community, broadcast, licence]\n",
      "1      [fire, witness, must, aware, defamation]\n",
      "2    [call, infrastructure, protection, summit]\n",
      "3                   [staff, aust, strike, rise]\n",
      "4      [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3]) \n",
    "\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43354634",
   "metadata": {},
   "source": [
    "## 4-1 tf-idf 행렬 만들기(1000개)\n",
    "\n",
    "- sklearn 함수인 TfidfVectorizer는 입력 데이터로 단어 집합이 아니라 문장을 사용함: 정제된 단어들에 대해 detokenize를 수행하여 문장을 다시 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43eddb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_9632/2844421375.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = detokenized_doc # 다시 text['headline_text']에 재저장\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 역토큰화 (토큰화 작업을 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc # 다시 text['headline_text']에 재저장\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 상위 1,000개의 단어를 보존\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000)\n",
    "\n",
    "X = vectorizer.fit_transform(text['headline_text']) \n",
    "X.shape # TF-IDF 행렬의 크기 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d899a",
   "metadata": {},
   "source": [
    "## 4-2 tf-idf 행렬 만들기(3000개)\n",
    "\n",
    "- vectorizer_3000에 3000개의 단어를 보존하여, X_3000에 행렬 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be94d8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 3000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 3,000개의 단어를 보존, 4.1에서 역토큰화한 결과 활용\n",
    "vectorizer_3000 = TfidfVectorizer(stop_words='english', max_features= 3000)\n",
    "\n",
    "X_3000 = vectorizer_3000.fit_transform(text['headline_text']) \n",
    "# TF-IDF 행렬의 크기 확인 \n",
    "X_3000.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3f0f4",
   "metadata": {},
   "source": [
    "## 4-3 tf-idf 행렬 만들기(5000개)\n",
    "\n",
    "- vectorizer_5000에 5000개의 단어를 보존하여, X_5000에 행렬 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcfa74c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 1,000개의 단어를 보존\n",
    "vectorizer_5000 = TfidfVectorizer(stop_words='english', max_features= 5000)\n",
    "\n",
    "X_5000 = vectorizer_5000.fit_transform(text['headline_text']) \n",
    "X_5000.shape # TF-IDF 행렬의 크기 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08bbba1",
   "metadata": {},
   "source": [
    "## 5. 토픽 모델링\n",
    "\n",
    "- 1000개의 단어 토픽 모델링은 vectorizer와 X를 활용하여\n",
    "- 3000개의 단어 토픽 모델링은 vectorizer_3000와 X_3000를 활용하여\n",
    "- 5000개의 당어 토픽 모델링은 vectorizer_5000와 X_5000를 활용하여 \n",
    "\n",
    "토픽모델링을 진행하였고 아래와 같은 결과값을 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c547d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------1000개 단어----------------------------------------------\n",
      "Topic 1: [('government', 8725.19), ('sydney', 8393.29), ('queensland', 7720.12), ('change', 5874.27), ('home', 5674.38), ('year', 5586.42), ('live', 5488.62), ('arrest', 3934.91), ('final', 3652.78), ('bank', 3270.15)]\n",
      "Topic 2: [('australia', 13691.08), ('australian', 11088.95), ('melbourne', 7528.43), ('world', 6707.7), ('south', 6677.03), ('canberra', 6112.23), ('country', 5251.55), ('accuse', 4070.65), ('lose', 3633.43), ('dead', 3503.84)]\n",
      "Topic 3: [('death', 5935.06), ('interview', 5924.98), ('kill', 5851.6), ('jail', 4632.85), ('life', 4275.27), ('health', 4267.08), ('minister', 3973.22), ('leave', 3849.71), ('news', 3601.53), ('lead', 3545.11)]\n",
      "Topic 4: [('house', 6113.49), ('2016', 5488.19), ('state', 4923.41), ('brisbane', 4857.21), ('tasmania', 4610.97), ('miss', 4546.98), ('hospital', 4332.34), ('family', 3965.05), ('fight', 3872.94), ('concern', 3604.71)]\n",
      "Topic 5: [('court', 7542.74), ('attack', 6959.64), ('open', 5663.0), ('face', 5193.63), ('warn', 5115.01), ('years', 5080.75), ('shoot', 4492.07), ('turnbull', 4269.85), ('murder', 4226.63), ('people', 4121.07)]\n",
      "Topic 6: [('market', 5545.86), ('rural', 5502.89), ('plan', 4828.71), ('indigenous', 4223.4), ('power', 3968.26), ('price', 3818.24), ('water', 3734.76), ('fall', 3601.75), ('deal', 3513.48), ('high', 3427.66)]\n",
      "Topic 7: [('charge', 8428.8), ('election', 7561.63), ('adelaide', 6758.36), ('make', 5658.99), ('test', 5062.69), ('tasmanian', 4859.02), ('share', 4011.03), ('league', 3911.12), ('child', 3838.44), ('need', 3708.86)]\n",
      "Topic 8: [('police', 12092.44), ('crash', 5281.14), ('drug', 4290.87), ('beat', 3257.58), ('rise', 2934.92), ('want', 2906.66), ('head', 2691.83), ('turn', 2632.73), ('station', 2618.04), ('western', 2556.61)]\n",
      "Topic 9: [('fund', 4693.03), ('labor', 4047.69), ('national', 4038.68), ('council', 4006.62), ('claim', 3604.75), ('flood', 3440.7), ('park', 3397.91), ('farm', 3364.05), ('guilty', 3311.88), ('island', 3165.84)]\n",
      "Topic 10: [('trump', 11966.41), ('perth', 6456.53), ('report', 5611.33), ('school', 5465.06), ('woman', 5456.76), ('coast', 5429.41), ('help', 5225.56), ('north', 5142.38), ('china', 4533.32), ('women', 4232.53)]\n",
      "--------------------------------3000개 단어----------------------------------------------\n",
      "Topic 1: [('melbourne', 6123.81), ('south', 5489.87), ('open', 4393.29), ('accuse', 3279.73), ('fall', 2963.08), ('budget', 2931.44), ('work', 2881.09), ('health', 2875.98), ('victoria', 2811.34), ('city', 2505.54)]\n",
      "Topic 2: [('police', 9651.47), ('rise', 3298.73), ('death', 3196.13), ('flood', 2822.28), ('indigenous', 2726.02), ('beat', 2541.34), ('centre', 2487.28), ('search', 2441.13), ('talk', 2401.51), ('royal', 2358.97)]\n",
      "Topic 3: [('change', 4761.16), ('year', 4627.02), ('hour', 3432.46), ('time', 3186.76), ('dead', 2870.28), ('claim', 2801.94), ('lead', 2775.57), ('federal', 2626.23), ('release', 2483.45), ('help', 2223.02)]\n",
      "Topic 4: [('court', 5986.97), ('woman', 4548.92), ('face', 4114.78), ('test', 3981.04), ('price', 3124.0), ('fight', 3095.02), ('trial', 2914.05), ('child', 2909.81), ('farm', 2760.55), ('guilty', 2753.62)]\n",
      "Topic 5: [('attack', 5430.16), ('canberra', 4965.14), ('interview', 4439.72), ('coast', 4334.15), ('tasmanian', 3993.18), ('gold', 3089.53), ('service', 3030.97), ('final', 2861.95), ('lose', 2860.8), ('community', 2626.11)]\n",
      "Topic 6: [('trump', 9080.34), ('government', 7189.86), ('world', 5247.35), ('country', 4737.61), ('home', 4601.0), ('school', 4508.37), ('warn', 4104.63), ('drug', 3560.57), ('people', 3431.21), ('national', 3341.36)]\n",
      "Topic 7: [('australian', 8772.92), ('queensland', 6274.04), ('perth', 5235.21), ('house', 4980.23), ('market', 4706.06), ('north', 4260.99), ('brisbane', 4034.79), ('tasmania', 3808.37), ('miss', 3664.85), ('west', 3315.57)]\n",
      "Topic 8: [('sydney', 6729.77), ('report', 4519.95), ('live', 4355.77), ('donald', 3721.05), ('plan', 3415.89), ('power', 3252.55), ('minister', 3126.5), ('leave', 3104.18), ('news', 2847.05), ('return', 2711.12)]\n",
      "Topic 9: [('election', 6091.44), ('adelaide', 5572.41), ('rural', 4813.94), ('2016', 4543.96), ('make', 4408.25), ('crash', 4356.76), ('state', 4089.57), ('shoot', 3653.04), ('hospital', 3605.45), ('women', 3456.99)]\n",
      "Topic 10: [('australia', 7824.37), ('charge', 6917.13), ('kill', 4673.94), ('years', 4167.26), ('jail', 3755.43), ('china', 3679.36), ('life', 3435.01), ('league', 3274.94), ('arrest', 3194.5), ('murder', 3160.97)]\n",
      "--------------------------------5000개 단어----------------------------------------------\n",
      "Topic 1: [('australian', 8122.21), ('government', 6740.12), ('attack', 5057.59), ('world', 4832.76), ('country', 4584.93), ('tasmania', 3572.61), ('miss', 3393.17), ('labor', 3082.25), ('minister', 2901.5), ('news', 2749.02)]\n",
      "Topic 2: [('trump', 8500.1), ('adelaide', 5189.49), ('home', 4299.32), ('make', 3991.47), ('tasmanian', 3737.44), ('bank', 2567.5), ('return', 2484.5), ('federal', 2482.57), ('victorian', 2468.07), ('centre', 2317.92)]\n",
      "Topic 3: [('court', 5553.79), ('market', 4440.27), ('report', 4192.57), ('face', 3817.26), ('brisbane', 3747.09), ('hour', 3410.47), ('share', 3116.5), ('rise', 3111.54), ('plan', 3054.87), ('trial', 2657.95)]\n",
      "Topic 4: [('election', 5713.22), ('school', 4185.79), ('open', 4070.83), ('years', 3841.02), ('jail', 3442.16), ('women', 3215.75), ('people', 3198.9), ('life', 3149.46), ('arrest', 2978.23), ('police', 2968.07)]\n",
      "Topic 5: [('melbourne', 5722.01), ('change', 4449.62), ('year', 4296.24), ('live', 4140.6), ('crash', 4100.45), ('donald', 3504.51), ('hospital', 3318.38), ('national', 3161.71), ('time', 2902.23), ('leave', 2832.15)]\n",
      "Topic 6: [('perth', 4882.88), ('kill', 4367.42), ('2016', 4280.73), ('interview', 3816.45), ('china', 3453.33), ('accuse', 3063.33), ('child', 2973.11), ('beat', 2371.49), ('near', 2322.92), ('push', 2134.25)]\n",
      "Topic 7: [('house', 4597.83), ('north', 3987.88), ('test', 3722.37), ('shoot', 3400.21), ('west', 3121.76), ('power', 3008.69), ('family', 2969.32), ('budget', 2776.09), ('drum', 2771.06), ('help', 2747.22)]\n",
      "Topic 8: [('australia', 7419.87), ('queensland', 5826.46), ('police', 5084.35), ('woman', 4252.05), ('coast', 4087.4), ('warn', 3807.55), ('death', 3546.59), ('drug', 3262.8), ('2015', 3110.45), ('league', 3108.2)]\n",
      "Topic 9: [('south', 5141.07), ('canberra', 4653.86), ('turnbull', 3159.39), ('lose', 2643.41), ('farm', 2542.89), ('city', 2346.79), ('search', 2235.9), ('violence', 2212.56), ('hold', 2200.38), ('port', 2081.42)]\n",
      "Topic 10: [('charge', 6435.89), ('sydney', 4755.74), ('rural', 4616.43), ('state', 3835.01), ('indigenous', 3171.72), ('murder', 3130.99), ('fall', 2771.6), ('need', 2699.67), ('high', 2658.73), ('claim', 2595.18)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 단어 집합. 1,000개의 단어가 저장됨.\n",
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1) \n",
    "lda_top=lda_model.fit_transform(X)\n",
    "terms = vectorizer.get_feature_names() \n",
    "\n",
    "# 단어 집합. 3,000개의 단어가 저장됨.\n",
    "lda_model_3000=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1) \n",
    "lda_top_3000=lda_model_3000.fit_transform(X_3000)\n",
    "terms_3000 = vectorizer_3000.get_feature_names()\n",
    "\n",
    "# 단어 집합. 5,000개의 단어가 저장됨\n",
    "lda_model_5000=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1) \n",
    "lda_top_5000=lda_model_5000.fit_transform(X_5000)\n",
    "terms_5000 = vectorizer_5000.get_feature_names() \n",
    "\n",
    "# 출력 함수 \n",
    "def get_topics(components, feature_names, n=10):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "print(\"--------------------------------1000개 단어----------------------------------------------\")\n",
    "get_topics(lda_model.components_,terms)\n",
    "print(\"--------------------------------3000개 단어----------------------------------------------\")\n",
    "get_topics(lda_model_3000.components_,terms_3000)\n",
    "print(\"--------------------------------5000개 단어----------------------------------------------\")\n",
    "get_topics(lda_model_5000.components_,terms_5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
